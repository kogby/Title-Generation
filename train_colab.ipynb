{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kogby/Title-Generation/blob/main/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ1J0lyCTuMD"
      },
      "source": [
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4eJnXx36TuME",
        "outputId": "ac0b5c1e-fe2b-484f-c8ae-a443fe5327b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Title-Generation'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 54 (delta 8), reused 14 (delta 5), pack-reused 33 (from 1)\u001b[K\n",
            "Receiving objects: 100% (54/54), 55.96 MiB | 39.41 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kogby/Title-Generation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4Y1b-SfHTuMF",
        "outputId": "8bc8ca9a-b894-4564-a340-b802c1f8acc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Title-Generation\n"
          ]
        }
      ],
      "source": [
        "cd Title-Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "skhpQ5zYTuMF",
        "outputId": "5d37936d-7c4a-4651-dbce-2a6d23e606f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在下載 punkt...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "下載完成！\n",
            "正在解壓 data.zip...\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: __MACOSX/._data         \n",
            "  inflating: data/train.jsonl        \n",
            "  inflating: __MACOSX/data/._train.jsonl  \n",
            "  inflating: data/sample_submission.jsonl  \n",
            "  inflating: __MACOSX/data/._sample_submission.jsonl  \n",
            "  inflating: data/sample_test.jsonl  \n",
            "  inflating: __MACOSX/data/._sample_test.jsonl  \n",
            "  inflating: data/public.jsonl       \n",
            "  inflating: __MACOSX/data/._public.jsonl  \n",
            "正在下載 best_checkpoint.zip...\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1a0eUu3p8M366JpTgYYvLwbG836TUo7rQ\n",
            "From (redirected): https://drive.google.com/uc?id=1a0eUu3p8M366JpTgYYvLwbG836TUo7rQ&confirm=t&uuid=1e2f30dd-abf6-4da0-9fc5-47459a3f2b15\n",
            "To: /content/Title-Generation/best_checkpoint.zip\n",
            "100% 938M/938M [00:15<00:00, 60.2MB/s]\n",
            "下載完成！\n",
            "正在解壓 best_checkpoint.zip...\n",
            "Archive:  best_checkpoint.zip\n",
            "   creating: best_checkpoint/\n",
            "  inflating: best_checkpoint/config.json  \n",
            "  inflating: best_checkpoint/generation_config.json  \n",
            "  inflating: best_checkpoint/pytorch_model.bin  \n",
            "  inflating: best_checkpoint/special_tokens_map.json  \n",
            "  inflating: best_checkpoint/spiece.model  \n",
            "  inflating: best_checkpoint/tokenizer.json  \n",
            "  inflating: best_checkpoint/tokenizer_config.json  \n"
          ]
        }
      ],
      "source": [
        "!bash download.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZzpdueZTuMG"
      },
      "source": [
        "# Install Requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "BR86x-I3TuMG",
        "outputId": "d4d553a9-665c-49c8-af79-b5351a329e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.21.0 in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.21.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.21.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.21.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.21.0) (0.2.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e82d85fdee94d8c91a46fa80b7edecae71efeb2aa22fa9d206db539b9c388286\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets==2.21.0\n",
        "!pip install evaluate\n",
        "!pip install rouge\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4W9YHOTuMG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "cYAWDSUtTuMH",
        "outputId": "7362fc0a-73b9-4fb2-a403-b44ed649934c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "完成切分，訓練資料 18453 條，驗證資料 3257 條\n",
            "2024-10-24 14:15:03.253868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-24 14:15:03.270839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-24 14:15:03.291638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-24 14:15:03.298010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-24 14:15:03.313194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-24 14:15:04.566888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/24/2024 14:15:06 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Generating train split: 18453 examples [00:01, 15203.01 examples/s]\n",
            "Generating validation split: 3257 examples [00:00, 17478.23 examples/s]\n",
            "loading configuration file best_checkpoint/config.json\n",
            "Model config MT5Config {\n",
            "  \"_name_or_path\": \"best_checkpoint\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading file spiece.model\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading weights file best_checkpoint/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at best_checkpoint.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file best_checkpoint/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset: 100% 18453/18453 [00:07<00:00, 2400.83 examples/s]\n",
            "Running tokenizer on dataset: 100% 3257/3257 [00:01<00:00, 2407.02 examples/s]\n",
            "10/24/2024 14:15:20 - INFO - __main__ - Sample 16972 of the training set: {'input_ids': [259, 68176, 179750, 4002, 16332, 150362, 16384, 85899, 4491, 72543, 30504, 17481, 138058, 17355, 146222, 152091, 190291, 40504, 493, 156711, 4366, 240469, 103889, 239061, 11215, 7605, 261, 3722, 169598, 100242, 35635, 261, 236372, 236372, 51061, 21270, 129113, 493, 173984, 51253, 30504, 261, 23584, 261, 4235, 41297, 130773, 291, 219076, 129113, 12901, 150362, 16384, 146626, 5991, 3541, 3203, 261, 180471, 217671, 428, 74602, 939, 150362, 16384, 18555, 239818, 30504, 879, 261, 848, 848, 1083, 187652, 21263, 137266, 138058, 13462, 18839, 1644, 9411, 197052, 15159, 292, 201128, 493, 129113, 24783, 29419, 34202, 103102, 261, 939, 98173, 23808, 12901, 493, 7544, 261, 29505, 201286, 12901, 28936, 879, 261, 26145, 131272, 24801, 129113, 19438, 12901, 150362, 16384, 493, 451, 1146, 31624, 261, 49110, 144907, 155895, 12901, 48734, 150362, 16384, 309, 259, 240393, 4938, 79056, 10256, 241677, 45670, 8320, 39240, 26518, 275, 186694, 21270, 10639, 267, 230513, 56010, 241400, 191679, 259, 31624, 4490, 1326, 259, 14783, 1322, 1193, 243036, 1193, 259, 26828, 13605, 150362, 16384, 24134, 47694, 18915, 5638, 11269, 5507, 6929, 18904, 33692, 259, 72543, 30504, 6072, 150362, 16384, 5507, 26553, 27722, 493, 27611, 1374, 230163, 261, 10170, 205896, 7153, 25458, 111026, 120775, 82195, 261, 47694, 7431, 7605, 86303, 1637, 37219, 121906, 15101, 194762, 292, 19583, 239492, 1146, 106217, 292, 100147, 4366, 22565, 63607, 160924, 292, 33810, 21270, 33810, 6072, 493, 182204, 122376, 101197, 292, 115385, 63607, 1146, 29630, 2395, 261, 173984, 65056, 25456, 18803, 86303, 1543, 939, 15862, 241301, 879, 493, 111934, 1193, 261, 111026, 120775, 20075, 24134, 939, 123009, 34202, 18803, 54172, 23584, 129113, 12901, 150362, 16384, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 178003, 12901, 15972, 26387, 13891, 309, 259, 129113, 939, 19438, 12901, 150362, 16384, 879, 493, 451, 1146, 31624, 259, 49110, 144907, 4794, 12901, 48734, 1]}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:616: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "10/24/2024 14:17:01 - INFO - __main__ - ***** Running training *****\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Num examples = 18453\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Num Epochs = 1\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "10/24/2024 14:17:01 - INFO - __main__ -   Total optimization steps = 577\n",
            "100% 577/577 [03:21<00:00,  3.09it/s]"
          ]
        }
      ],
      "source": [
        "!bash train.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS268sFjTuMH"
      },
      "source": [
        "# Infer Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "iH7-HMEGTuMI",
        "outputId": "49236f23-b24c-4c3b-c8d8-299a70bdf5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/checkpoint/resolve/main/tokenizer_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1855, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1752, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-671a34fb-79fd70895cbf09ad1a435517;639bc6ce-bd77-439a-896c-aa17760a014a)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/checkpoint/resolve/main/tokenizer_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Title-Generation/inference.py\", line 102, in <module>\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 834, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 666, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 425, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: checkpoint is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        }
      ],
      "source": [
        "!bash run.sh data/public.jsonl submission.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h5C3bLsTuMI"
      },
      "source": [
        "# Do Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "lkA2mvfQTuMI",
        "outputId": "572774c3-8d05-4ced-b62f-d710324286ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Title-Generation/eval.py\", line 3, in <module>\n",
            "    from tw_rouge import get_rouge\n",
            "ImportError: cannot import name 'get_rouge' from 'tw_rouge' (unknown location)\n"
          ]
        }
      ],
      "source": [
        "# !bash eval.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "WoHY5i3HYWyp",
        "outputId": "21f9ecb9-469f-42f2-c592-150fb10fe6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 271 bytes | 271.00 KiB/s, done.\n",
            "From https://github.com/kogby/Title-Generation\n",
            "   ace9a07..ec7e181  main       -> origin/main\n",
            "Updating ace9a07..ec7e181\n",
            "Fast-forward\n",
            " train.sh | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "_blF-dugYrOm",
        "outputId": "97a071b0-4701-4643-e17b-43da87db5fd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_checkpoint      data.zip\t  eval.sh\tpreprocess.py  train_checkpoint   train.sh  wandb\n",
            "best_checkpoint.zip  download.sh  inference.py\tREADME.md      train_colab.ipynb  tw_rouge\n",
            "data\t\t     eval.py\t  __MACOSX\trun.sh\t       train.py\t\t  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r train_checkpoint.zip train_checkpoint"
      ],
      "metadata": {
        "id": "D_7gr1V50z3W",
        "outputId": "47efd706-c08a-4bda-a80c-9befb9da6ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: train_checkpoint/ (stored 0%)\n",
            "  adding: train_checkpoint/model.safetensors (deflated 22%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/ (stored 0%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/events.out.tfevents.1729777218.168d01fde21e.2692.0 (deflated 9%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777358.3052177/ (stored 0%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777358.3052177/hparams.yml (deflated 53%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777511.4379456/ (stored 0%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777511.4379456/hparams.yml (deflated 53%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777358.3034072/ (stored 0%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777358.3034072/events.out.tfevents.1729777358.168d01fde21e.3308.1 (deflated 54%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/events.out.tfevents.1729777504.168d01fde21e.4265.0 (deflated 32%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/events.out.tfevents.1729777342.168d01fde21e.3308.0 (deflated 9%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777511.4362762/ (stored 0%)\n",
            "  adding: train_checkpoint/summarization_no_trainer/1729777511.4362762/events.out.tfevents.1729777511.168d01fde21e.4265.1 (deflated 54%)\n",
            "  adding: train_checkpoint/spiece.model (deflated 46%)\n",
            "  adding: train_checkpoint/tokenizer_config.json (deflated 68%)\n",
            "  adding: train_checkpoint/tokenizer.json (deflated 76%)\n",
            "  adding: train_checkpoint/generation_config.json (deflated 30%)\n",
            "  adding: train_checkpoint/special_tokens_map.json (deflated 73%)\n",
            "  adding: train_checkpoint/config.json (deflated 47%)\n",
            "  adding: train_checkpoint/all_results.json (deflated 37%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "fga0Skyi1B50",
        "outputId": "7dc771f7-a4ba-4c17-9718-8494f75df498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_checkpoint      download.sh   __MACOSX\t  train_checkpoint\ttrain.sh\n",
            "best_checkpoint.zip  eval.py\t   preprocess.py  train_checkpoint.zip\ttw_rouge\n",
            "data\t\t     eval.sh\t   README.md\t  train_colab.ipynb\tutils.py\n",
            "data.zip\t     inference.py  run.sh\t  train.py\t\twandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhZbDKjh2HJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}